{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gezmi/ColabFold/blob/main/score_mpdockq_AlphaFold2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4yBrceuFbf3"
      },
      "source": [
        "#ColabFold: AlphaFold2 w/ MMseqs2 BATCH\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sokrypton/ColabFold/main/.github/ColabFold_Marv_Logo_Small.png\" height=\"256\" align=\"right\" style=\"height:256px\">\n",
        "\n",
        "Easy to use AlphaFold2 protein structure [(Jumper et al. 2021)](https://www.nature.com/articles/s41586-021-03819-2) and complex [(Evans et al. 2021)](https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1) prediction using multiple sequence alignments generated through MMseqs2. For details, refer to our manuscript:\n",
        "\n",
        "[Mirdita M, Schütze K, Moriwaki Y, Heo L, Ovchinnikov S, Steinegger M. ColabFold: Making protein folding accessible to all.\n",
        "*Nature Methods*, 2022](https://www.nature.com/articles/s41592-022-01488-1) \n",
        "\n",
        "**Usage**\n",
        "\n",
        "`input_dir` directory with only fasta files or MSAs stored in Google Drive. MSAs need to be A3M formatted and have an `.a3m` extention. For MSAs MMseqs2 will not be called.\n",
        "\n",
        "`result_dir` results will be written to the result directory in Google Drive\n",
        "\n",
        "Old versions: [v1.0-alpha](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.0-alpha/batch/AlphaFold2_batch.ipynb), [v1.1-permultimer](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.1-premultimer//batch/AlphaFold2_batch.ipynb), [v1.2](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.2.0/batch/AlphaFold2_batch.ipynb), [v1.3](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.3.0/batch/AlphaFold2_batch.ipynb)\n",
        "\n",
        "<strong>For more details, see <a href=\"#Instructions\">bottom</a> of the notebook and checkout the [ColabFold GitHub](https://github.com/sokrypton/ColabFold). </strong>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Scoring functions\n",
        "# from Github repo: https://gitlab.com/ElofssonLab/FoldDock/-/blob/main/src/pdockq.py\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import pdb\n",
        "import glob\n",
        "\n",
        "#####################FUNCTIONS#########################\n",
        "def parse_atm_record(line):\n",
        "    '''Get the atm record\n",
        "    '''\n",
        "    record = defaultdict()\n",
        "    record['name'] = line[0:6].strip()\n",
        "    record['atm_no'] = int(line[6:11])\n",
        "    record['atm_name'] = line[12:16].strip()\n",
        "    record['atm_alt'] = line[17]\n",
        "    record['res_name'] = line[17:20].strip()\n",
        "    record['chain'] = line[21]\n",
        "    record['res_no'] = int(line[22:26])\n",
        "    record['insert'] = line[26].strip()\n",
        "    record['resid'] = line[22:29]\n",
        "    record['x'] = float(line[30:38])\n",
        "    record['y'] = float(line[38:46])\n",
        "    record['z'] = float(line[46:54])\n",
        "    record['occ'] = float(line[54:60])\n",
        "    record['B'] = float(line[60:66])\n",
        "\n",
        "    return record\n",
        "\n",
        "def read_pdb_pdockq(pdbfile):\n",
        "    '''Read a pdb file predicted with AF and rewritten to conatin all chains\n",
        "    '''\n",
        "\n",
        "    chain_coords, chain_plddt = {}, {}\n",
        "    with open(pdbfile, 'r') as file:\n",
        "        for line in file:\n",
        "            if not line.startswith('ATOM'):\n",
        "                continue\n",
        "            record = parse_atm_record(line)\n",
        "            #Get CB - CA for GLY\n",
        "            if record['atm_name']=='CB' or (record['atm_name']=='CA' and record['res_name']=='GLY'):\n",
        "                if record['chain'] in [*chain_coords.keys()]:\n",
        "                    chain_coords[record['chain']].append([record['x'],record['y'],record['z']])\n",
        "                    chain_plddt[record['chain']].append(record['B'])\n",
        "                else:\n",
        "                    chain_coords[record['chain']] = [[record['x'],record['y'],record['z']]]\n",
        "                    chain_plddt[record['chain']] = [record['B']]\n",
        "\n",
        "\n",
        "    #Convert to arrays\n",
        "    for chain in chain_coords:\n",
        "        chain_coords[chain] = np.array(chain_coords[chain])\n",
        "        chain_plddt[chain] = np.array(chain_plddt[chain])\n",
        "\n",
        "    return chain_coords, chain_plddt\n",
        "\n",
        "\n",
        "def read_pdb_mpdockq(pdbfile):\n",
        "    '''Read a pdb file per chain\n",
        "    '''\n",
        "    pdb_chains = {}\n",
        "    chain_coords = {}\n",
        "    chain_CA_inds = {}\n",
        "    chain_CB_inds = {}\n",
        "    chain_plddt = {}\n",
        "\n",
        "    with open(pdbfile) as file:\n",
        "        for line in file:\n",
        "          if line[0:6] == 'ATOM  ':\n",
        "            record = parse_atm_record(line)\n",
        "            if record['chain'] in [*pdb_chains.keys()]:\n",
        "                pdb_chains[record['chain']].append(line)\n",
        "                chain_coords[record['chain']].append([record['x'],record['y'],record['z']])\n",
        "                coord_ind+=1\n",
        "                if record['atm_name']=='CA':\n",
        "                    chain_CA_inds[record['chain']].append(coord_ind)\n",
        "                    chain_plddt[record['chain']].append(record['B'])\n",
        "                if record['atm_name']=='CB' or (record['atm_name']=='CA' and record['res_name']=='GLY'):\n",
        "                    chain_CB_inds[record['chain']].append(coord_ind)\n",
        "\n",
        "            else:\n",
        "                pdb_chains[record['chain']] = [line]\n",
        "                chain_coords[record['chain']]= [[record['x'],record['y'],record['z']]]\n",
        "                chain_CA_inds[record['chain']]= []\n",
        "                chain_CB_inds[record['chain']]= []\n",
        "                chain_plddt[record['chain']]= []\n",
        "                #Reset coord ind\n",
        "                coord_ind = 0\n",
        "\n",
        "    return pdb_chains, chain_coords, chain_CA_inds, chain_CB_inds, chain_plddt\n",
        "\n",
        "def read_plddt_mpdockq(plddtdir, chain_lens, model_path):\n",
        "    '''Get the plDDT for each chain\n",
        "    '''\n",
        "\n",
        "    plddt_per_chain = {}\n",
        "    for ind, row in model_path.iterrows():\n",
        "        source_plDDT =  np.load(plddtdir+row.Source+'.npy')\n",
        "        si = 0\n",
        "        for p_chain in row.Source.split('_')[-1]:\n",
        "            if p_chain==row.Chain:\n",
        "                plddt_per_chain[row.Chain]=source_plDDT[si:si+chain_lens[row.Chain]]\n",
        "                break\n",
        "            else:\n",
        "                si += chain_lens[p_chain]\n",
        "\n",
        "    #Get the last chain\n",
        "    missing_chain = np.setdiff1d([*chain_lens.keys()], [*plddt_per_chain.keys()])[0]\n",
        "    row = model_path[model_path.Edge_chain==missing_chain]\n",
        "    source_plDDT =  np.load(plddtdir+row.Source.values[0]+'.npy')\n",
        "    si = 0\n",
        "    for p_chain in row.Source.values[0].split('_')[-1]:\n",
        "        if p_chain==missing_chain:\n",
        "            plddt_per_chain[missing_chain]=source_plDDT[si:si+chain_lens[missing_chain]]\n",
        "            break\n",
        "        else:\n",
        "            si += chain_lens[p_chain]\n",
        "\n",
        "    return plddt_per_chain\n",
        "\n",
        "\n",
        "def score_complex(path_coords, path_CB_inds, path_plddt):\n",
        "    '''Score all interfaces in the current complex\n",
        "    '''\n",
        "    metrics = {'Chain':[], 'n_ints':[], 'sum_av_IF_plDDT':[],\n",
        "                'n_contacts':[], 'n_IF_residues':[]}\n",
        "\n",
        "    chains = [*path_coords.keys()]\n",
        "    chain_inds = np.arange(len(chains))\n",
        "    #Get interfaces per chain\n",
        "    for i in chain_inds:\n",
        "        chain_i = chains[i]\n",
        "        chain_coords = np.array(path_coords[chain_i])\n",
        "        chain_CB_inds = path_CB_inds[chain_i]\n",
        "        l1 = len(chain_CB_inds)\n",
        "        chain_CB_coords = chain_coords[chain_CB_inds]\n",
        "        chain_plddt = np.array(path_plddt[chain_i])\n",
        "        #Metrics\n",
        "        n_chain_ints = 0\n",
        "        chain_av_IF_plDDT = 0\n",
        "        n_chain_contacts = 0\n",
        "        n_chain_IF_residues = 0\n",
        "\n",
        "        for int_i in np.setdiff1d(chain_inds, i):\n",
        "            int_chain = chains[int_i]\n",
        "            int_chain_CB_coords = np.array(path_coords[int_chain])[path_CB_inds[int_chain]]\n",
        "            int_chain_plddt = np.array(path_plddt[int_chain])\n",
        "            #Calc 2-norm\n",
        "            mat = np.append(chain_CB_coords,int_chain_CB_coords,axis=0)\n",
        "            a_min_b = mat[:,np.newaxis,:] -mat[np.newaxis,:,:]\n",
        "            dists = np.sqrt(np.sum(a_min_b.T ** 2, axis=0)).T\n",
        "            contact_dists = dists[:l1,l1:]\n",
        "            contacts = np.argwhere(contact_dists<=8)\n",
        "            #The first axis contains the contacts from chain 1\n",
        "            #The second the contacts from chain 2\n",
        "            if contacts.shape[0]>0:\n",
        "                n_chain_ints += 1\n",
        "                chain_av_IF_plDDT +=  np.concatenate((chain_plddt[contacts[:,0]], int_chain_plddt[contacts[:,1]])).mean()\n",
        "                n_chain_contacts += contacts.shape[0]\n",
        "                n_chain_IF_residues += np.unique(contacts).shape[0]\n",
        "\n",
        "        #Save\n",
        "        metrics['Chain'].append(chain_i)\n",
        "        metrics['n_ints'].append(n_chain_ints)\n",
        "        metrics['sum_av_IF_plDDT'].append(chain_av_IF_plDDT) #Divide with n_ints to get avg per int\n",
        "        metrics['n_contacts'].append(n_chain_contacts)\n",
        "        metrics['n_IF_residues'].append(n_chain_IF_residues)\n",
        "    #Create df\n",
        "    metrics_df = pd.DataFrame.from_dict(metrics)\n",
        "    return metrics_df\n",
        "\n",
        "\n",
        "def calc_pdockq(chain_coords, chain_plddt, t):\n",
        "    '''Calculate the pDockQ scores\n",
        "    pdockQ = L / (1 + np.exp(-k*(x-x0)))+b\n",
        "    L= 0.724 x0= 152.611 k= 0.052 and b= 0.018\n",
        "    '''\n",
        "\n",
        "    #Get coords and plddt per chain\n",
        "    ch1, ch2 = [*chain_coords.keys()]\n",
        "    coords1, coords2 = chain_coords[ch1], chain_coords[ch2]\n",
        "    plddt1, plddt2 = chain_plddt[ch1], chain_plddt[ch2]\n",
        "\n",
        "    #Calc 2-norm\n",
        "    mat = np.append(coords1, coords2,axis=0)\n",
        "    a_min_b = mat[:,np.newaxis,:] -mat[np.newaxis,:,:]\n",
        "    dists = np.sqrt(np.sum(a_min_b.T ** 2, axis=0)).T\n",
        "    l1 = len(coords1)\n",
        "    contact_dists = dists[:l1,l1:] #upper triangular --> first dim = chain 1\n",
        "    contacts = np.argwhere(contact_dists<=t)\n",
        "\n",
        "    if contacts.shape[0]<1:\n",
        "        pdockq=0\n",
        "        ppv=0\n",
        "        contact_dists = []\n",
        "        avg_if_plddt = 0\n",
        "        x = 0\n",
        "    else:\n",
        "        #Get the average interface plDDT\n",
        "        avg_if_plddt = np.average(np.concatenate([plddt1[np.unique(contacts[:,0])], plddt2[np.unique(contacts[:,1])]]))\n",
        "        #Get the number of interface contacts\n",
        "        n_if_contacts = contacts.shape[0]\n",
        "        x = avg_if_plddt*np.log10(n_if_contacts)\n",
        "        pdockq = 0.724 / (1 + np.exp(-0.052*(x-152.611)))+0.018\n",
        "\n",
        "        #PPV\n",
        "        PPV = np.array([0.98128027, 0.96322524, 0.95333044, 0.9400192 ,\n",
        "            0.93172991, 0.92420274, 0.91629946, 0.90952562, 0.90043139,\n",
        "            0.8919553 , 0.88570037, 0.87822061, 0.87116417, 0.86040801,\n",
        "            0.85453785, 0.84294946, 0.83367787, 0.82238224, 0.81190228,\n",
        "            0.80223507, 0.78549007, 0.77766077, 0.75941223, 0.74006263,\n",
        "            0.73044282, 0.71391784, 0.70615739, 0.68635536, 0.66728511,\n",
        "            0.63555449, 0.55890174])\n",
        "\n",
        "        pdockq_thresholds = np.array([0.67333079, 0.65666073, 0.63254566, 0.62604391,\n",
        "            0.60150931, 0.58313803, 0.5647381 , 0.54122438, 0.52314392,\n",
        "            0.49659878, 0.4774676 , 0.44661346, 0.42628389, 0.39990988,\n",
        "            0.38479715, 0.3649393 , 0.34526004, 0.3262589 , 0.31475668,\n",
        "            0.29750023, 0.26673725, 0.24561247, 0.21882689, 0.19651314,\n",
        "            0.17606258, 0.15398168, 0.13927677, 0.12024131, 0.09996019,\n",
        "            0.06968505, 0.02946438])\n",
        "        inds = np.argwhere(pdockq_thresholds>=pdockq)\n",
        "        if len(inds)>0:\n",
        "            ppv = PPV[inds[-1]][0]\n",
        "        else:\n",
        "            ppv = PPV[0]\n",
        "\n",
        "    return pdockq, ppv, contact_dists, avg_if_plddt, x\n",
        "\n",
        "def calc_mpDockQ(metrics_df):\n",
        "    '''Calculats the multiple interface pDockQ\n",
        "    '''\n",
        "\n",
        "    def sigmoid(x, L ,x0, k, b):\n",
        "        y = L / (1 + np.exp(-k*(x-x0)))+b\n",
        "        return y\n",
        "\n",
        "\n",
        "    av_IF_plDDT = np.average(metrics_df.sum_av_IF_plDDT/metrics_df.n_ints)\n",
        "    n_contacts= metrics_df.n_contacts.sum()\n",
        "\n",
        "    L = 0.783\n",
        "    x0= 289.79\n",
        "    k= 0.061\n",
        "    b= 0.23\n",
        "    mpDockQ = sigmoid(av_IF_plDDT*np.log10(n_contacts+0.001), L ,x0, k, b)\n",
        "\n",
        "    return mpDockQ\n",
        "\n",
        "def process_pdb(pdb_path, pdockq = False):\n",
        "  model_id = os.path.splitext(os.path.basename(pdb_path))[0]\n",
        "  pdb_chains, chain_coords, chain_CA_inds, chain_CB_inds, chain_plddt = read_pdb_mpdockq(pdb_path)\n",
        "\n",
        "  #Check chains\n",
        "  if len(chain_coords.keys())<2:\n",
        "    print('Only one chain in pdbfile', pdb_path)\n",
        "    return None, None, None\n",
        "  elif len(chain_coords.keys())>2 or pdockq == False:\n",
        "    metrics_df = score_complex(chain_coords, chain_CB_inds, chain_plddt)\n",
        "    #Add id\n",
        "    metrics_df['ID'] = model_id\n",
        "    #Calc mpDockQ\n",
        "    mpDockQ = calc_mpDockQ(metrics_df)\n",
        "    metrics_df['mpDockQ'] = mpDockQ\n",
        "\n",
        "    return(metrics_df)\n",
        "  else:\n",
        "    #Calculate pdockq\n",
        "    t = 8 #Distance threshold, set to 8 Å\n",
        "    chain_coords, chain_plddt = read_pdb_pdockq(pdb_path)\n",
        "    pdockq, ppv, contact_dists, avg_if_plddt, x = calc_pdockq(chain_coords, chain_plddt, t)\n",
        "    #print('pDockQ =',np.round(pdockq,3),'for',pdb_path)\n",
        "    #print('This corresponds to a PPV of at least', ppv)\n",
        "    metrics = {'Chain':[], 'n_ints':[], 'sum_av_IF_plDDT':[],\n",
        "            'n_contacts':[], 'n_IF_residues':[]}\n",
        "    metrics['Chain'].append(None)\n",
        "    metrics['n_ints'].append(n_chain_ints)\n",
        "    metrics['sum_av_IF_plDDT'].append(avg_if_plddt) #Divide with n_ints to get avg per int\n",
        "    metrics['n_contacts'].append(n_chain_contacts)\n",
        "    metrics['n_IF_residues'].append(n_chain_IF_residues)\n",
        "    metrics['ID'].append(model_id)\n",
        "    return pdockq, ppv, avg_if_plddt\n",
        "\n",
        "# process_pdb('ufl1_nterm_ufc1_ad088_unrelaxed_rank_1_model_1.pdb')"
      ],
      "metadata": {
        "id": "1AUoBhwhrwOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r7Xkbsi25ZPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwvIWN3HDyUJ",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23eccd60-942a-4a63-a69d-956daf06a813"
      },
      "source": [
        "#@title Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOblAo-xetgx",
        "cellView": "form"
      },
      "source": [
        "#@title Input protein sequence, then hit `Runtime` -> `Run all`\n",
        "\n",
        "input_dir = '/content/drive/MyDrive/input_fasta' #@param {type:\"string\"}\n",
        "result_dir = '/content/drive/MyDrive/ufl1_pulldown_output' #@param {type:\"string\"}\n",
        "\n",
        "# number of models to use\n",
        "#@markdown ---\n",
        "#@markdown ### Advanced settings\n",
        "msa_mode = \"MMseqs2 (UniRef+Environmental)\" #@param [\"MMseqs2 (UniRef+Environmental)\", \"MMseqs2 (UniRef only)\",\"single_sequence\",\"custom\"]\n",
        "num_models = 5 #@param [1,2,3,4,5] {type:\"raw\"}\n",
        "num_recycles = 3 #@param [1,3,6,12,24,48] {type:\"raw\"}\n",
        "stop_at_score = 100 #@param {type:\"string\"}\n",
        "#@markdown - early stop computing models once score > threshold (avg. plddt for \"structures\" and ptmscore for \"complexes\")\n",
        "use_custom_msa = False\n",
        "use_amber = False #@param {type:\"boolean\"}\n",
        "use_templates = False #@param {type:\"boolean\"}\n",
        "do_not_overwrite_results = True #@param {type:\"boolean\"}\n",
        "#zip_results = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iccGdbe_Pmt9",
        "cellView": "form"
      },
      "source": [
        "#@title Install dependencies\n",
        "%%bash -s $use_amber $use_templates\n",
        "\n",
        "set -e\n",
        "\n",
        "USE_AMBER=$1\n",
        "USE_TEMPLATES=$2\n",
        "\n",
        "\n",
        "if [ ! -f COLABFOLD_READY ]; then\n",
        "  # install dependencies\n",
        "  # We have to use \"--no-warn-conflicts\" because colab already has a lot preinstalled with requirements different to ours\n",
        "  pip install -q --no-warn-conflicts \"colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold\"\n",
        "  # high risk high gain\n",
        "  pip install -q \"jax[cuda11_cudnn805]>=0.3.8,<0.4\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "  touch COLABFOLD_READY\n",
        "fi\n",
        "\n",
        "# Download params (~1min)\n",
        "python -m colabfold.download\n",
        "\n",
        "# setup conda\n",
        "if [ ${USE_AMBER} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
        "  if [ ! -f CONDA_READY ]; then\n",
        "    wget -qnc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "    bash Miniconda3-latest-Linux-x86_64.sh -bfp /usr/local 2>&1 1>/dev/null\n",
        "    rm Miniconda3-latest-Linux-x86_64.sh\n",
        "    touch CONDA_READY\n",
        "  fi\n",
        "fi\n",
        "# setup template search\n",
        "if [ ${USE_TEMPLATES} == \"True\" ] && [ ! -f HH_READY ]; then\n",
        "  conda install -y -q -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python=3.7 2>&1 1>/dev/null\n",
        "  touch HH_READY\n",
        "fi\n",
        "# setup openmm for amber refinement\n",
        "if [ ${USE_AMBER} == \"True\" ] && [ ! -f AMBER_READY ]; then\n",
        "  conda install -y -q -c conda-forge openmm=7.5.1 python=3.7 pdbfixer 2>&1 1>/dev/null\n",
        "  touch AMBER_READY\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUYApPElB30u",
        "cellView": "form"
      },
      "source": [
        "#@title Run Prediction\n",
        "import sys\n",
        "from colabfold.batch import get_queries, run\n",
        "from colabfold.download import default_data_dir\n",
        "from colabfold.utils import setup_logging\n",
        "from pathlib import Path\n",
        "\n",
        "# For some reason we need that to get pdbfixer to import\n",
        "if use_amber and '/usr/local/lib/python3.7/site-packages/' not in sys.path:\n",
        "    sys.path.insert(0, '/usr/local/lib/python3.7/site-packages/')\n",
        "\n",
        "if 'logging_setup' not in globals():\n",
        "    setup_logging(Path(result_dir).joinpath(\"log.txt\"))\n",
        "    logging_setup = True\n",
        "\n",
        "queries, is_complex = get_queries(input_dir)\n",
        "run(\n",
        "    queries=queries,\n",
        "    result_dir=result_dir,\n",
        "    use_templates=use_templates,\n",
        "    use_amber=use_amber,\n",
        "    msa_mode=msa_mode,\n",
        "    model_type=\"auto\",\n",
        "    num_models=num_models,\n",
        "    num_recycles=num_recycles,\n",
        "    model_order=[3, 4, 5, 1, 2],\n",
        "    is_complex=is_complex,\n",
        "    data_dir=default_data_dir,\n",
        "    keep_existing_results=do_not_overwrite_results,\n",
        "    rank_by=\"auto\",\n",
        "    pair_mode=\"unpaired+paired\",\n",
        "    stop_at_score=stop_at_score,\n",
        "    zip_results=zip_results,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Score with mpdockq\n",
        "scores = []\n",
        "for pdb_file in glob.glob(os.path.join(result_dir, '*rank*.pdb')):\n",
        "  \n",
        "\n",
        "  #pdockq, ppv, avg_if_plddt = process_pdb(pdb_file)\n",
        "  scores.append([basename, pdockq, ppv, avg_if_plddt])\n",
        "  break\n",
        "score_df = pd.DataFrame(scores)\n",
        "score_df.columns = ['model_name', 'pdockq', 'PPV', 'avg_if_plddt']\n",
        "score_df.to_csv(os.path.join(result_dir, 'mpdockq_scores.csv'))\n",
        "score_df"
      ],
      "metadata": {
        "id": "RIe-oLCgtAfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGUBLzB3C6WN"
      },
      "source": [
        "# Instructions <a name=\"Instructions\"></a>\n",
        "**Quick start**\n",
        "1. Upload your single fasta files to a folder in your Google Drive\n",
        "2. Define path to the fold containing the fasta files (`input_dir`) define an outdir (`output_dir`)\n",
        "3. Press \"Runtime\" -> \"Run all\".\n",
        "\n",
        "**Result zip file contents**\n",
        "\n",
        "At the end of the job a all results `jobname.result.zip` will be uploaded to your (`output_dir`) Google Drive. Each zip contains one protein.\n",
        "\n",
        "1. PDB formatted structures sorted by avg. pIDDT. (unrelaxed and relaxed if `use_amber` is enabled).\n",
        "2. Plots of the model quality.\n",
        "3. Plots of the MSA coverage.\n",
        "4. Parameter log file.\n",
        "5. A3M formatted input MSA.\n",
        "6. BibTeX file with citations for all used tools and databases.\n",
        "\n",
        "\n",
        "**Troubleshooting**\n",
        "* Check that the runtime type is set to GPU at \"Runtime\" -> \"Change runtime type\".\n",
        "* Try to restart the session \"Runtime\" -> \"Factory reset runtime\".\n",
        "* Check your input sequence.\n",
        "\n",
        "**Known issues**\n",
        "* Google Colab assigns different types of GPUs with varying amount of memory. Some might not have enough memory to predict the structure for a long sequence.\n",
        "* Google Colab assigns different types of GPUs with varying amount of memory. Some might not have enough memory to predict the structure for a long sequence.\n",
        "* Your browser can block the pop-up for downloading the result file. You can choose the `save_to_google_drive` option to upload to Google Drive instead or manually download the result file: Click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
        "\n",
        "**Limitations**\n",
        "* Computing resources: Our MMseqs2 API can handle ~20-50k requests per day.\n",
        "* MSAs: MMseqs2 is very precise and sensitive but might find less hits compared to HHblits/HMMer searched against BFD or Mgnify.\n",
        "* We recommend to additionally use the full [AlphaFold2 pipeline](https://github.com/deepmind/alphafold).\n",
        "\n",
        "**Description of the plots**\n",
        "*   **Number of sequences per position** - We want to see at least 30 sequences per position, for best performance, ideally 100 sequences.\n",
        "*   **Predicted lDDT per position** - model confidence (out of 100) at each position. The higher the better.\n",
        "*   **Predicted Alignment Error** - For homooligomers, this could be a useful metric to assess how confident the model is about the interface. The lower the better.\n",
        "\n",
        "**Bugs**\n",
        "- If you encounter any bugs, please report the issue to https://github.com/sokrypton/ColabFold/issues\n",
        "\n",
        "**License**\n",
        "\n",
        "The source code of ColabFold is licensed under [MIT](https://raw.githubusercontent.com/sokrypton/ColabFold/main/LICENSE). Additionally, this notebook uses AlphaFold2 source code and its parameters licensed under [Apache 2.0](https://raw.githubusercontent.com/deepmind/alphafold/main/LICENSE) and  [CC BY 4.0](https://creativecommons.org/licenses/by-sa/4.0/) respectively. Read more about the AlphaFold license [here](https://github.com/deepmind/alphafold).\n",
        "\n",
        "**Acknowledgments**\n",
        "- We thank the AlphaFold team for developing an excellent model and open sourcing the software. \n",
        "\n",
        "- [Söding Lab](https://www.mpibpc.mpg.de/soeding) for providing the computational resources for the MMseqs2 server\n",
        "\n",
        "- Do-Yoon Kim for creating the ColabFold logo.\n",
        "\n",
        "- A colab by Sergey Ovchinnikov ([@sokrypton](https://twitter.com/sokrypton)), Milot Mirdita ([@milot_mirdita](https://twitter.com/milot_mirdita)) and Martin Steinegger ([@thesteinegger](https://twitter.com/thesteinegger)).\n"
      ]
    }
  ]
}